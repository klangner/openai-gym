{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letâ€™s make a DQN: Debugging\n",
    "\n",
    "https://gym.openai.com/envs/MountainCar-v0/\n",
    "\n",
    "Some useful resources:\n",
    "  * https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from replay_buffer import ReplayBuffer\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explore environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (2,)\n",
      "Number of actions: 2\n",
      "Example state: [-0.54055684  0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x145abe9b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spawn game instance for tests\n",
    "env = gym.make(\"MountainCar-v0\").env #create raw env\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = 2 # We simplify to left and right action\n",
    "\n",
    "print(\"Observation shape: {}\".format(observation_shape))\n",
    "print(\"Number of actions: {}\".format(n_actions))\n",
    "\n",
    "print(\"Example state: {}\".format(env.reset()))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. How many random steps are needed to solve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of steps: 16801.3 (std=15607.085269517816)\n"
     ]
    }
   ],
   "source": [
    "def solve_randomly():\n",
    "    env.reset()\n",
    "    for step in range(100_000):\n",
    "        a = np.random.choice([0, 2])\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        if done:\n",
    "            break\n",
    "    return step\n",
    "            \n",
    "## Look for 10 solution using random policy\n",
    "solution_steps = []\n",
    "for _ in range(10):\n",
    "    steps = solve_randomly()\n",
    "    solution_steps.append(steps)\n",
    "    \n",
    "print('Mean number of steps: {} (std={})'.format(np.mean(solution_steps), np.std(solution_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare\n",
    "\n",
    "## 2.1. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0003\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "LAMBDA = 0.001      # speed of the decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Helper functions\n",
    "\n",
    "### 2.2.1. Normalize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode action\n",
    "def encode_action(a):\n",
    "    return a if a == 0 else 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2. Memory buffer and training set\n",
    "\n",
    "#FIXME: Check if reward is calculated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "def build_training_set(qvalues, qvalues_next, actions, rewards, dones, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Create training set for QNetwork.\n",
    "    Params:\n",
    "      qvalues           - Q values for the starting state\n",
    "      qvalues_next      - Q values for the state the next state\n",
    "      actions           - Actions taken\n",
    "      rewards           - Rewards received after taking action \n",
    "      dones             - Did this action end the episode?\n",
    "      \n",
    "    Returns:\n",
    "      Expected qvalues\n",
    "    \"\"\"\n",
    "    y = qvalues.copy()\n",
    "    next_rewards = np.where(dones, np.zeros(rewards.shape), np.max(qvalues_next, axis=1))\n",
    "    y[np.arange(y.shape[0]), actions] = rewards + gamma * next_rewards\n",
    "    return y\n",
    "\n",
    "\n",
    "# Some tests\n",
    "qvalues = np.zeros((5, n_actions))\n",
    "qvalues2 = np.ones((5, n_actions))\n",
    "actions = np.array([0, 1, 0, 1, 0])\n",
    "rewards = np.array([1, 2, 3, 4, 5])\n",
    "dones = np.array([False, False, False, False, True])\n",
    "expected_y = np.array([[2, 0], [0, 3], [4, 0], [0, 5], [5, 0]])\n",
    "y = build_training_set(qvalues, qvalues2, actions, rewards, dones, 1.0)\n",
    "assert np.array_equal(y, expected_y), 'Wrong expected qvalue calculated'\n",
    "print('Ok!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. DQN Agent\n",
    "\n",
    "### 2.3.1. Define Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10341555, 0.08274443], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QNetwork:\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions, alpha=LEARNING_RATE):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.model = self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.InputLayer(self.input_shape))\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(self.output_shape, activation='linear'))\n",
    "        opt = optimizers.RMSprop(lr=self.alpha)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"Make prediction for single state and return q values for all actions\"\"\"\n",
    "        s = np.expand_dims(state, axis=0)\n",
    "        return self.model.predict(s)[0]\n",
    "    \n",
    "    def predict_batch(self, states):\n",
    "        \"\"\"Make prediction for list of states\"\"\"\n",
    "        return self.model.predict(states)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        self.model.fit(x, y, batch_size=64, verbose=0)\n",
    "\n",
    "        \n",
    "network = QNetwork(observation_shape, n_actions)\n",
    "network.predict(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, network, replays=ReplayBuffer(REPLAY_BUFFER_SIZE)):\n",
    "        self.memory_capacity = 100000\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_max = 0.01\n",
    "        self.epsilon_lambda = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = self.epsilon_max        \n",
    "        self.model = network\n",
    "        self.replays = replays\n",
    "        self.step = 0\n",
    "        \n",
    "    def collect_policy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(0, network.output_shape)\n",
    "        else:\n",
    "            qvalues = self.model.predict(state)\n",
    "            action = np.argmax(qvalues)\n",
    "            self.step += 1\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max-self.epsilon_min) * np.power(np.e, self.epsilon_lambda*self.step)\n",
    "        return action\n",
    "    \n",
    "    def add_observation(self, state, action, reward, next_state, is_done):\n",
    "        self.replays.add(state, action, reward, next_state, is_done)\n",
    "        \n",
    "    def train(self):\n",
    "        states, actions, rewards, states_next, dones = self.replays.sample(self.batch_size)\n",
    "        qvalues = self.model.predict_batch(states)\n",
    "        qvalues_next = self.model.predict_batch(states_next)\n",
    "        y = build_training_set(qvalues, qvalues_next, actions, rewards, dones)\n",
    "        self.model.train(states, y)\n",
    "    \n",
    "    \n",
    "network = QNetwork(observation_shape, n_actions)\n",
    "agent = DQNAgent(network)\n",
    "s1 = env.reset()\n",
    "a = agent.policy(s1)\n",
    "s2, r, d, _ = env.step(a)\n",
    "agent.add_observation(s1, a, r, s2, d)\n",
    "agent.train()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_on_episode(env, agent, t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        action = agent.collect_policy(state)\n",
    "        encoded_action = encode_action(action)\n",
    "        next_state, reward, done, info = env.step(encoded_action)\n",
    "        agent.add_observation(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def train_agent(env, agent, max_episodes=2000, t_max=200, solved=199.0):\n",
    "\n",
    "    #generate new sessions\n",
    "    rewards = []\n",
    "    for i in range(1, max_episodes+1):\n",
    "        session_reward = train_on_episode(env, agent, t_max)\n",
    "        rewards.append(session_reward)\n",
    "        if i % 100 == 0:\n",
    "            mean_score = np.mean(rewards[-100:])\n",
    "            clear_output(True)\n",
    "            print('Step: {}, mean reward: {}'.format(i, mean_score))\n",
    "            plt.plot(rewards)\n",
    "            plt.show()\n",
    "            if mean_score > solved:\n",
    "                print(\"Solved in {} steps\".format(i))\n",
    "                break\n",
    "    return rewards\n",
    "\n",
    "# Test\n",
    "agent = RandomAgent(n_actions)\n",
    "train_agent(env, agent, max_episodes=1, t_max=100, solved=-90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Debugging\n",
    "\n",
    "### 3.1. Generate episodes using random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved 9 times\n"
     ]
    }
   ],
   "source": [
    "replays = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "env.reset()\n",
    "dones = 0\n",
    "for step in range(REPLAY_BUFFER_SIZE):\n",
    "    a = np.random.choice([0, 2])\n",
    "    s_next, r, done, info = env.step(a)\n",
    "    if done:\n",
    "        dones += 1\n",
    "        env.reset()\n",
    "            \n",
    "print('Solved {} times'.format(dones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train inital agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Q function visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Value function visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
